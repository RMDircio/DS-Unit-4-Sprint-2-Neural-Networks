{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "-----------------------------------------------------\n",
    "**This article is very helpful [Towards Data Science](https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a) and [TDS: Neuron](https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90) and [TDS: Perception](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53)**\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "![Image 1:](https://miro.medium.com/max/500/1*3fA77_mLNiJTSgZFhYnU0Q.png)*Neural Network Architecture*\n",
    "### **Input Layer:**\n",
    "        The initial data for the neural network. Usually are noted as\n",
    "$vector X$\n",
    "        \n",
    "### **Hidden Layer:**\n",
    "        The intermediate layer between input and output layer and place where all the computation is done. Normally represent the “activation” nodes and usually are noted as\n",
    "$W$ or\n",
    "$θ$.\n",
    "        \n",
    "### **Output Layer:**\n",
    "        Produces the result for given inputs. Also known as the predicted value (or values in case of multiple output classes/types).\n",
    "### **Neuron:**\n",
    "    Since Nerural Networks are inspired by the human brain, the building blocks of NN are Neurons. It takes in some inputs and fires an output. In machine learning it is placeholder for a mathematical function, and its only job is to provide an output by applying the function on the inputs provided.\n",
    "### **Weight:**\n",
    "        Each node is connected with each node from the next layer and each connection (black arrow) has particular weight. Weight impacts the node from the next layer. Each weight can be thought of the strength of a node.\n",
    "\n",
    "![Image 2:](https://miro.medium.com/max/587/1*uz3wd5YeVYlU2JR8rE9VDA.png)*Node from Neural Network*\n",
    "### **Activation Function:**\n",
    "    Defines if given node should be “activated” or not based on the weighted sum. There are difference kinds of AF: Step Function, Linear Function, Sigmoid Function. They are used to map the input between the required values - (0,1) or (-1,1). Acts like a control valve. We use the dirvative of the function to update the weights.\n",
    "#### **Step Function:**\n",
    "    Discrete output values.\n",
    "    \n",
    "\\begin{align}\n",
    "if(z > threshold) — “activate” the node (value 1)<br>if(z < threshold) — don’t “activate” the node (value 0)\n",
    "\\end{align}\n",
    "\n",
    "#### **Linear Function:**\n",
    "    A range of output values. But we are not able to map any non-linear data.\n",
    "\n",
    "#### **Sigmoid Function:**\n",
    "    Most widely used.\n",
    "![Image 9](https://miro.medium.com/max/225/1*Exx_40zOAvrSV3mP6_aTcw.png)*Sigmoid Equation* <br>\n",
    "![Image 10](https://miro.medium.com/max/500/1*jSCPkJo0ZpBRA5H3JqFhQg.png)*Sigmoid Function*\n",
    "\n",
    "#### Common AF\n",
    "![Activation Functions](http://www.snee.com/bobdc.blog/img/activationfunctions.png)\n",
    "### **Bias Node:**\n",
    "     Critical for creating successful learning model, since it a bias value allows for shifting the Activation Function left or right. This helps get a better fit for the data, hence a better prediction function as the output. In other words, once can shift the activation function curve up or down.\n",
    "### **Node Map:**\n",
    "    A visual interpretation of the movement of information in Neural Networks.\n",
    "![Image 3](https://miro.medium.com/max/700/1*pR6bSQXgC1Y_lYTeLDqVqg.png)*Simple Neural Network*\n",
    "### **Perceptron:**\n",
    "    Perceptron is a **single layer neural network** and a **multi-layer perceptron is called Neural Networks.** It is a linear classifier - binary. Used in supervised learning to classify the data into two parts.\n",
    "    Perception has four parts:\n",
    "    1. Input values or One input layer\n",
    "    2. Weights and Bias\n",
    "    3. Net sum\n",
    "    4. Activation Function\n",
    "![Perception](https://miro.medium.com/max/700/1*n6sJ4yZQzwKL9wnF5wnVNg.png)*Perception*\n",
    "\n",
    "**a. All the inputs x are multiplied with their weights w. Let’s call it k.** <br>\n",
    "![Inputs+Weights](https://miro.medium.com/max/700/1*_Zy1C83cnmYUdETCeQrOgA.png)*Multiplying inputs with weights for 5 inputs*\n",
    "\n",
    "**b. Add all the multiplied values and call them Weighted Sum.** <br>\n",
    "![Adding](https://miro.medium.com/max/528/1*xFd9VQnUM1H0kiCENsoYxg.gif)*Adding with Summation*\n",
    "\n",
    "**c. Apply that weighted sum to the correct Activation Function.** <br>\n",
    "![Step AF](https://miro.medium.com/max/480/1*0iOzeMS3s-3LTU9hYH9ryg.png)*Step Activation Function*\n",
    "    \n",
    "![NN with pictures](https://miro.medium.com/max/700/1*eEKb2RxREV6-MtLz2DNWFQ.gif)*Cat or Dog?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "\n",
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### First the data is taken in **Inputs** and then in the **Hidden Layer** the computaion is done. Finally the **Output** is given after the computation is compelte. To make this process more complex, each layer - input/hidden/output - have nodes who have given weights. These weights play an important role, they identify which nodes will activate with in the computational stage - Hidden Layer - this is done with activation functions. Bias will allow for shifts in the curve, thus creating better predictions.\n",
    "\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Letcure material did a learning rate of 1, this will mess up the material here, so I need to insert a learning rate parameter = 0.1 or 0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        weights = ...\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "\n",
    "            # Activate!\n",
    "\n",
    "            # Cac error\n",
    "\n",
    "            # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
